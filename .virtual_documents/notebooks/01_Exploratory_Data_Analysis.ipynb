import pandas as pd
# Load the data
data = pd.read_csv('../data/boiler_data.csv') 

# Display the data
print("Data loaded successfully! Here are the first 5 rows:")
print(data.head())


import matplotlib.pyplot as plt
import seaborn as sns

# Set up a visual style
sns.set_style("whitegrid")

# Create a chart to compare Run Hours vs. Vibration, colored by Failure
plt.figure(figsize=(10, 6))
sns.scatterplot(x='run_hours', y='vibration', hue='failure_flag', data=data, 
                palette={0: 'blue', 1: 'red'}, style='failure_flag', s=100)
plt.title('Run Hours vs. Vibration (Colored by Failure)')
plt.xlabel('Run Hours')
plt.ylabel('Vibration')
plt.legend(title='Failure (1=Yes)')
plt.show()


# Calculate the correlation matrix
correlation_matrix = data.corr(numeric_only=True)

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title('Feature Correlation Matrix')
plt.show()

# Print the correlation of each feature with the target (failure_flag)
print("\n--- Correlation with Failure Flag ---")
print(correlation_matrix['failure_flag'].sort_values(ascending=False))


from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score
import numpy as np
import pandas as pd # Ensure pandas is imported

# Note: The data is already loaded, but we re-load it to be safe in a fresh session
data = pd.read_csv('../data/boiler_data.csv')

# --- 1. Define the Rule (Heuristic Baseline) ---
# True (1) if Run Hours is 1500 or more AND Vibration is greater than 2.0
rule_prediction = np.where((data['run_hours'] >= 1500) & (data['vibration'] > 2.0), 1, 0)

# --- 2. Calculate Metrics ---
actual_outcomes = data['failure_flag']
cm = confusion_matrix(actual_outcomes, rule_prediction)

# Calculate the key metrics
rule_accuracy = accuracy_score(actual_outcomes, rule_prediction)
rule_precision = precision_score(actual_outcomes, rule_prediction, zero_division=0)
rule_recall = recall_score(actual_outcomes, rule_prediction, zero_division=0)
rule_f1 = f1_score(actual_outcomes, rule_prediction, zero_division=0)

# --- 3. Output Results (Kairo's Request) ---
print("--- Heuristic Baseline Metrics (The Rule) ---")
print(f"Accuracy: {rule_accuracy:.2f}")
print(f"Precision: {rule_precision:.2f}")
print(f"Recall: {rule_recall:.2f}")
print(f"F1 Score: {rule_f1:.2f}")
print("\nConfusion Matrix:")
print(cm)


import matplotlib.pyplot as plt 
import seaborn as sns 
import numpy as np 
import os 
from sklearn.metrics import confusion_matrix

# --- Re-calculating all necessary variables (THE FIX) ---
# Assuming 'data' is loaded from a previous cell run
y_true = data['failure_flag']
y_pred_rule = np.where((data['run_hours'] >= 1500) & (data['vibration'] > 2.0), 1, 0)
cm = confusion_matrix(y_true, y_pred_rule) # <-- This calculation was missing!


os.makedirs("figures", exist_ok=True) # Makes sure the 'figures' folder exists
labels = ["No Failure", "Failure"] 

plt.figure(figsize=(4.5,4)) 
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", 
            xticklabels=labels, yticklabels=labels, 
            cbar=False, linewidths=0.5, linecolor="gray") 
plt.title("Heuristic Rule — Confusion Matrix") 
plt.xlabel("Predicted") 
plt.ylabel("Actual") 
plt.tight_layout() 
plt.savefig("figures/rule_confusion_matrix.png", dpi=200) 
plt.show()


from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# 1. Define Features (X) and Target (y)
# (Using the same features as Logistic Regression: run_hours, temp_avg, pressure_avg, vibration)
X = data.drop(['failure_flag', 'sensor_id'], axis=1) 
y = data['failure_flag']

# 2. Perform a Stratified Train/Test Split (Kairo's Request)
# Stratify ensures a balanced mix of 0s and 1s in the train/test sets, which is crucial 
# since our dataset is small and perfectly balanced (5 failures, 5 non-failures).
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Train set size: {X_train.shape[0]} samples")
print(f"Test set size: {X_test.shape[0]} samples")


# 1. Train the Decision Tree Model (with Kairo's specified parameters)
dt_model = DecisionTreeClassifier(random_state=42, class_weight="balanced")
dt_model.fit(X_train, y_train)

# 2. Make Predictions
dt_prediction = dt_model.predict(X_test)

# 3. Output Evaluation Metrics (Kairo's Request)
print("--- Decision Tree Model Evaluation ---")
print("\nClassification Report:\n", classification_report(y_test, dt_prediction, zero_division=0))
print("\nTest Accuracy:", accuracy_score(y_test, dt_prediction))

# 4. Generate Confusion Matrix (for plotting later)
dt_cm = confusion_matrix(y_test, dt_prediction)
print("\nConfusion Matrix:\n", dt_cm)


from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os

# --- 1. Define Features (X) and Target (y) ---
# Assuming 'data' is loaded from a previous cell run
X = data.drop(['failure_flag', 'sensor_id'], axis=1) 
y = data['failure_flag']

# 2. Stratified Train/Test Split (Kairo's Request)
# Note: stratify=y is CRITICAL for balancing the small test set.
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("--- Data Split Confirmation ---")
print(f"Train set size: {X_train.shape[0]} samples (Failures: {y_train.sum()})")
print(f"Test set size: {X_test.shape[0]} samples (Failures: {y_test.sum()})\n")


# --- 3. Heuristic Baseline on Holdout (X_test) ---
rule_prediction = np.where((X_test['run_hours'] >= 1500) & (X_test['vibration'] > 2.0), 1, 0)
cm_rule = confusion_matrix(y_test, rule_prediction)

rule_accuracy = accuracy_score(y_test, rule_prediction)
rule_precision = precision_score(y_test, rule_prediction, zero_division=0)
rule_recall = recall_score(y_test, rule_prediction, zero_division=0)
rule_f1 = f1_score(y_test, rule_prediction, zero_division=0)


# --- 4. Plot and Save Confusion Matrix (Heuristic) ---
os.makedirs("figures", exist_ok=True)
labels = ["No Failure", "Failure"]

plt.figure(figsize=(4.5, 4))
sns.heatmap(cm_rule, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=labels, yticklabels=labels, linewidths=0.5, linecolor="gray")
plt.title('Heuristic Rule — Holdout CM')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.savefig("figures/heuristic_holdout_cm.png", dpi=200)
plt.show()

print("\n--- Heuristic Holdout Metrics ---")
print(f"Accuracy: {rule_accuracy:.2f}")
print(f"Precision: {rule_precision:.2f}")
print(f"Recall: {rule_recall:.2f}")


from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

# --- 1. TRAIN and PREDICT (Logistic Regression) ---
lr_model = LogisticRegression(solver='liblinear', random_state=42)
lr_model.fit(X_train, y_train)
lr_prediction = lr_model.predict(X_test)

# --- 2. TRAIN and PREDICT (Decision Tree) ---
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
dt_prediction = dt_model.predict(X_test)

# --- 3. Compile and Compare Metrics ---
results = {
    'Heuristic': {
        'Accuracy': rule_accuracy, 'Precision': rule_precision, 'Recall': rule_recall, 'F1 Score': rule_f1
    },
    'Logistic Regression': {
        'Accuracy': accuracy_score(y_test, lr_prediction), 
        'Precision': precision_score(y_test, lr_prediction, zero_division=0),
        'Recall': recall_score(y_test, lr_prediction, zero_division=0),
        'F1 Score': f1_score(y_test, lr_prediction, zero_division=0)
    },
    'Decision Tree': {
        'Accuracy': accuracy_score(y_test, dt_prediction), 
        'Precision': precision_score(y_test, dt_prediction, zero_division=0),
        'Recall': recall_score(y_test, dt_prediction, zero_division=0),
        'F1 Score': f1_score(y_test, dt_prediction, zero_division=0)
    }
}

# Convert to DataFrame for Kairo's requested comparison
metrics_df = pd.DataFrame(results).T.round(2)
print("\n--- Generalization Check Metrics (on X_test) ---")
print(metrics_df)






